{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4: Na√Øve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import math \n",
    "import pandas as pd\n",
    "import string\n",
    "import heapq\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetdf = pd.read_csv (r'Tweets.csv')\n",
    "tweetdf.columns = ['a_s','text']\n",
    "\n",
    "neutraltag = tweetdf[tweetdf['a_s'] == 'neutral']\n",
    "positivetag = tweetdf[tweetdf['a_s'] == 'positive']\n",
    "negativetag = tweetdf[tweetdf['a_s'] == 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_train = neutraltag[0:int( len(neutraltag)*0.8 )]\n",
    "positive_train = positivetag[0:int( len(positivetag)*0.8 )]\n",
    "negative_train = negativetag[0:int( len(negativetag)*0.8 )]    \n",
    "neutral_test = neutraltag[int(len(neutraltag)*0.8): len(neutraltag)]\n",
    "positive_test = positivetag[int(len(positivetag)*0.8): len(positivetag)]\n",
    "negative_test = negativetag[int(len(negativetag)*0.8): len(negativetag)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.concatenate((neutral_train['text'], positive_train['text'], negative_train['text']))\n",
    "testX = np.concatenate((neutral_test['text'], positive_test['text'], negative_test['text']))    \n",
    "trainY = np.concatenate((neutral_train['a_s'], positive_train['a_s'], negative_train['a_s']))\n",
    "testY = np.concatenate((neutral_test['a_s'], positive_test['a_s'], negative_test['a_s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "arraytrain = np.array([neutral_train['text'], positive_train['text'], negative_train['text']])\n",
    "arraytest = np.array([neutral_test['text'], positive_test['text'], negative_test['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#onehotencoding\n",
    "onehotencoded = pd.get_dummies(tweetdf.a_s)\n",
    "\n",
    "#labelencoding\n",
    "def labelEncoding(dataframe, newarray):\n",
    "    newarray=[]\n",
    "    for labels in dataframe:\n",
    "        if labels == 'neutral':\n",
    "            newarray.append(0)\n",
    "        elif labels == 'positive':\n",
    "            newarray.append(1)\n",
    "        elif labels == 'negative':\n",
    "            newarray.append(2)\n",
    "    newarray = np.asarray(newarray)\n",
    "    return newarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now to process X data, which is the text data, we need to represent it as a bag of words and define a dictionary with reduced\n",
    "#vocabulary size \n",
    "import re\n",
    "# def LowerCase(lines):\n",
    "#         text = text.lower()\n",
    "        \n",
    "# def RemoveNonWordAndDoubleSpace(lines):\n",
    "#         text = re.sub(r'\\W',' ',text)\n",
    "#         text = re.sub(r'\\s+',' ',text)\n",
    "        \n",
    "# array1 = []\n",
    "# def CleanedDataSet(lines, array):\n",
    "#     for text in lines:\n",
    "#         LowerCase(lines)\n",
    "#         RemoveNonWordAndDoubleSpace(lines)\n",
    "#         array1.append(text)\n",
    "#     array1 = pd.DataFrame(array1, columns=[\"text\"])\n",
    "#     return array1\n",
    "\n",
    "def CleanedDataSet(data):\n",
    "    empty =[]\n",
    "    for text in data:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\W',' ',text)\n",
    "        text = re.sub(r'\\s+',' ',text)\n",
    "        empty.append(text)\n",
    "    empty = pd.DataFrame(empty, columns=[\"text\"])\n",
    "    return empty\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Frequent words for bag of words \n",
    "def findfrequency(tweets):\n",
    "    wordfreq = []\n",
    "    mostfreq_stop = []\n",
    "    for t in tweets:\n",
    "        words = t.split()\n",
    "        for w in words:\n",
    "            if w not in wordfreq:\n",
    "                wordfreq.append(w)\n",
    "    mostfreq_stop = [word for word in wordfreq if not word in stopwords.words()]\n",
    "    return mostfreq_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "\n",
    "def bagofwords(tweets, totalwords):\n",
    "    vectorTweets=[]\n",
    "    for t in tweets:\n",
    "        TokenedTweets = t.split()\n",
    "        processedTweets=[]\n",
    "        for tw in totalwords:\n",
    "            if tw in TokenedTweets:\n",
    "                processedTweets.append(t.count(tw))\n",
    "            else:\n",
    "                processedTweets.append(0)\n",
    "        vectorTweets.append(processedTweets)\n",
    "    return vectorTweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing X and Y datas. Y train was not needed as OHE was not needed\n",
    "Y_Test = labelEncoding(testY, Y_Test)   \n",
    "Final_X_Train = []\n",
    "X_Train = CleanedDataSet(trainX)\n",
    "X_Test = CleanedDataSet(testX)\n",
    "mostfreq_stop = findfrequency(X_Train.text)\n",
    "\n",
    "\n",
    "Final_X_Train.append(CleanedDataSet(arraytrain[0]))\n",
    "Final_X_Train.append(CleanedDataSet(arraytrain[1]))\n",
    "Final_X_Train.append(CleanedDataSet(arraytrain[2]))\n",
    "\n",
    "Final_X_Train[0] = bagofwords(Final_X_Train[0].text, mostfreq_stop)\n",
    "Final_X_Train[0] = pd.DataFrame(Final_X_Train[0])\n",
    "Final_X_Train[0] = Final_X_Train[0].to_numpy()\n",
    "Final_X_Train[1] = bagofwords(Final_X_Train[1].text, mostfreq_stop)\n",
    "Final_X_Train[1] = pd.DataFrame(Final_X_Train[1])\n",
    "Final_X_Train[1] = Final_X_Train[1].to_numpy()\n",
    "Final_X_Train[2] = bagofwords(Final_X_Train[2].text, mostfreq_stop)\n",
    "Final_X_Train[2] = pd.DataFrame(Final_X_Train[2])\n",
    "Final_X_Train[2] = Final_X_Train[2].to_numpy()\n",
    "\n",
    "X_Test = bagofwords(X_Test.text, mostfreq_stop)\n",
    "X_Test = pd.DataFrame(X_Test)    \n",
    "X_Test = X_Test.to_numpy()\n",
    "\n",
    "X_Train = Final_X_Train\n",
    "X_Test = X_Test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNaiveBayes(trainX, X_Train):\n",
    "    \n",
    "    N_documents = (trainX.shape)[0]\n",
    "    N_neutral = len(X_Train[0])\n",
    "    N_positive = len(X_Train[1])\n",
    "    N_negative = len(X_Train[2])\n",
    "    prob_prior_neutral = N_neutral / N_documents\n",
    "    prob_prior_positive = N_positive / N_documents\n",
    "    prob_prior_negative = N_negative / N_documents\n",
    "    log_prior_neutral = np.log(prob_prior_neutral)\n",
    "    log_prior_positive = np.log(prob_prior_positive)\n",
    "    log_prior_negative = np.log(prob_prior_negative)\n",
    "    log_prior = np.array([log_prior_neutral, log_prior_positive, log_prior_negative])\n",
    "    \n",
    "    class_sum = []\n",
    "    class_total = []\n",
    "    likelihood = []\n",
    "    \n",
    "    neutral_sum = np.sum(X_Train[0], axis = 0)\n",
    "    positive_sum = np.sum(X_Train[1], axis = 0)\n",
    "    negative_sum = np.sum(X_Train[2], axis = 0)\n",
    "    neutral_total = np.sum(neutral_sum)\n",
    "    positive_total = np.sum(positive_sum)\n",
    "    negative_total = np.sum(negative_sum)\n",
    "    N_features = len(neutral_sum)\n",
    "\n",
    "    neutral_sum = neutral_sum + 1   \n",
    "    neutral_total = neutral_total + N_features\n",
    "    positive_sum = positive_sum + 1   \n",
    "    positive_total = positive_total + N_features\n",
    "    negative_sum = negative_sum + 1   \n",
    "    negative_total = negative_total + N_features\n",
    "    \n",
    "    neutral_prob = neutral_sum / neutral_total\n",
    "    positive_prob = positive_sum / positive_total\n",
    "    negative_prob = negative_sum / negative_total\n",
    "    log_prob = np.array([np.log(neutral_prob), np.log(positive_prob), np.log(negative_prob)])\n",
    "\n",
    "    return log_prior, log_prob\n",
    "\n",
    "log_prior, log_prob = trainNaiveBayes(trainX, X_Train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNaiveBayes(log_prior, log_prob, X_Test):\n",
    "    predictions = []\n",
    "    \n",
    "    test_prob_neutral = np.dot(X_Test,log_prob[0])\n",
    "    test_prob_neutral = test_prob_neutral + log_prior[0]\n",
    "    \n",
    "    test_prob_positive = np.dot(X_Test,log_prob[1])\n",
    "    test_prob_positive = test_prob_positive + log_prior[1]\n",
    "    \n",
    "    test_prob_negative = np.dot(X_Test,log_prob[2])\n",
    "    test_prob_negative = test_prob_negative + log_prior[2]\n",
    "    \n",
    "    m = len(X_Test)\n",
    "    bayes_output = []\n",
    "    \n",
    "    for i in range(m):\n",
    "        max_prob = max(test_prob_neutral[i], test_prob_positive[i], test_prob_negative[i])\n",
    "        temp = -1\n",
    "        if max_prob == test_prob_neutral[i]:\n",
    "                temp = 0\n",
    "        if max_prob == test_prob_positive[i]:\n",
    "                temp = 1\n",
    "        if max_prob == test_prob_negative[i]:\n",
    "                temp = 2\n",
    "        bayes_output.append(temp)\n",
    "                \n",
    "    return bayes_output\n",
    "\n",
    "prediction_label = testNaiveBayes(log_prior, log_prob, X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFinal(prediction, expected):\n",
    "    \n",
    "    truep = [0,0,0]\n",
    "    truen = [0,0,0]\n",
    "    falsep = [0,0,0]\n",
    "    faslen = [0,0,0]\n",
    "    confusion_matrix = np.full((3,3), 0)\n",
    "\n",
    "    for E, P in zip(expected, prediction): \n",
    "        confusion_matrix[P][E] = confusion_matrix[P][E] + 1\n",
    "        \n",
    "    \n",
    "    truep[0] = confusion_matrix[0][0]\n",
    "    falsep[0] = (confusion_matrix[:, 0]).sum() - truep[0]\n",
    "    faslen[0] = (confusion_matrix[0, :]).sum() - truep[0]\n",
    "    truen[0] = (confusion_matrix.sum()) - truep[0] - falsep[0] - faslen[0]\n",
    "    \n",
    "    truep[1] = confusion_matrix[1][1]\n",
    "    falsep[1] = (confusion_matrix[:, 1]).sum() - truep[1]\n",
    "    faslen[1] = (confusion_matrix[1, :]).sum() - truep[1]\n",
    "    truen[1] = (confusion_matrix.sum()) - truep[1] - falsep[1] - faslen[1]\n",
    "        \n",
    "    truep[2] = confusion_matrix[2][2]\n",
    "    falsep[2] = (confusion_matrix[:, 2]).sum() - truep[2]\n",
    "    faslen[2] = (confusion_matrix[2, :]).sum() - truep[2]\n",
    "    truen[2] = (confusion_matrix.sum()) - truep[2] - falsep[2] - faslen[2]\n",
    "    \n",
    "    print(confusion_matrix)\n",
    "    microA_precision = (sum(truep)/(sum(falsep)+sum(truep)))\n",
    "    microA_recall = (sum(truep)/(sum(faslen)+sum(truep)))\n",
    "    microA_accuracy = ((sum(truep)+sum(truen))/(sum(faslen)+sum(truep)+sum(falsep)+sum(truen)))\n",
    "    microA_f1score = (microA_precision * microA_recall) / (microA_precision + microA_recall)\n",
    "    microA_f1score = microA_f1score * 2\n",
    "    \n",
    "    macroA_precision = []\n",
    "    macroA_recall = []\n",
    "    macroA_accuracy = []\n",
    "    \n",
    "    macroA_precision.append(truep[0] / (truep[0] + falsep[0]))\n",
    "    macroA_recall.append(truep[0] / (truep[0] + faslen[0]))\n",
    "    macroA_accuracy.append((truep[0] + truen[0]) / (truep[0] + truen[0] + falsep[0] + faslen[0]))\n",
    "    \n",
    "    macroA_precision.append(truep[1] / (truep[1] + falsep[1]))\n",
    "    macroA_recall.append(truep[1] / (truep[1] + faslen[1]))\n",
    "    macroA_accuracy.append((truep[1] + truen[1]) / (truep[1] + truen[1] + falsep[1] + faslen[1]))\n",
    "    \n",
    "    macroA_precision.append(truep[2] / (truep[2] + falsep[2]))\n",
    "    macroA_recall.append(truep[2] / (truep[2] + faslen[2]))\n",
    "    macroA_accuracy.append((truep[2] + truen[2]) / (truep[2] + truen[2] + falsep[2] + faslen[2]))\n",
    "\n",
    "    sum_macroA_precision = (sum(macroA_precision))/3\n",
    "    sum_macroA_recall = (sum(macroA_recall))/3\n",
    "    sum_macroA_accuracy = (sum(macroA_accuracy))/3\n",
    "\n",
    "    macroA_f1score = (sum_macroA_precision * sum_macroA_recall) / (sum_macroA_precision + sum_macroA_recall)\n",
    "    macroA_f1score  = 2 * macroA_f1score \n",
    "    \n",
    "    print(\"\\nMicro Average Precision: \", microA_precision)\n",
    "    print(\"\\nMicro Average Recall: \", microA_recall)\n",
    "    print(\"\\nMicro Average Accuracy: \", microA_accuracy)\n",
    "    print(\"\\nMicro Average F1-Score: \", microA_f1score)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nMacro Average Precision: \", sum_macroA_precision)\n",
    "    print(\"\\nMacro Average Recall: \", sum_macroA_recall)\n",
    "    print(\"\\nMacro Average Accuracy: \", sum_macroA_accuracy)\n",
    "    print(\"\\nMacro Average F1-Score: \", macroA_f1score)\n",
    "\n",
    "evaluateFinal(prediction_label, Y_Test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "traindf=pd.read_csv('train.csv')\n",
    "testdf=pd.read_csv('test.csv')\n",
    "\n",
    "#From Previous Assignment: Label Encoding, where strings match the label, assign an integer and append to an array\n",
    "def labelTest(data):\n",
    "    petal_labels=[]\n",
    "    for i in data:\n",
    "        if i == 'Iris-setosa':\n",
    "            petal_labels.append(0)\n",
    "        if i == 'Iris-versicolor':\n",
    "            petal_labels.append(1)\n",
    "        if i == 'Iris-virginica':\n",
    "            petal_labels.append(2)\n",
    "    petal_labels = np.asarray(petal_labels)\n",
    "    return petal_labels\n",
    "petal_labels=labelTest(testdf[\"Class\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#finds square of each column entry of ith row between traindf and testdf. \n",
    "def euclidianDistance(data1, data2, length):\n",
    "    distance = 0\n",
    "    for i in range(length):\n",
    "        distance += np.square(data1[i] - data2[i])\n",
    "    distance = np.sqrt(distance)\n",
    "    return distance\n",
    "\n",
    "#Outer loop runs for a total of 15 test entries and inner loop runs for 135 times for train entries\n",
    "def SortedDistances(traindf, testdf):\n",
    "    UnSorted_distances = []\n",
    "    for i in range(len(testdf)):\n",
    "        test_row = testdf.iloc[i]\n",
    "        test_distances = []\n",
    "        \n",
    "        \n",
    "        for j in range(len(traindf)):\n",
    "            train_row = traindf.iloc[j]\n",
    "            test_distances.append((euclidianDistance(test_row, train_row, 4), train_row[4]))\n",
    "            \n",
    "        UnSorted_distances.append(test_distances)\n",
    "\n",
    "#the above funtion appends the distance found in a tuple with the label of the data being used. It makes an array of arrays \n",
    "#with unsorted tuples\n",
    "#the function below will sort the tuples within the array of arrays\n",
    "    sortDistances = []\n",
    "    for i in range(len(UnSorted_distances)):\n",
    "        sortDistances.append(sorted(UnSorted_distances[i],key=itemgetter(0)))\n",
    "    return sortDistances\n",
    "\n",
    "#find nearest K neighbors using mode values for labels\n",
    "def findNeighbors(sortDistances, k):\n",
    "    Neighborlabels = []\n",
    "    for i in range(len(sortDistances)):\n",
    "        nearest_k = []\n",
    "        for j in range(k):\n",
    "            nearest_k.append(sortDistances[i][j][1])\n",
    "        Neighborlabels.append(max(set(nearest_k), key=nearest_k.count))\n",
    "    return Neighborlabels\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(train, test, k):\n",
    "    sortDistances = SortedDistances(traindf, testdf)\n",
    "    labels = findNeighbors(sortDistances, 5)\n",
    "    predicts = labelTest(labels)\n",
    "    return predicts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = KNN(traindf, testdf,1)\n",
    "k3 = KNN(traindf, testdf,3)\n",
    "k5 = KNN(traindf, testdf,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For K = 1\n",
      "This is the confusion matrix: \n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "\n",
      "Micro Average Precision:  1.0\n",
      "\n",
      "Micro Average Recall:  1.0\n",
      "\n",
      "Micro Average Accuracy:  1.0\n",
      "\n",
      "Micro Average F1-Score:  1.0\n",
      "\n",
      "\n",
      "\n",
      "Macro Average Precision:  1.0\n",
      "\n",
      "Macro Average Recall:  1.0\n",
      "\n",
      "Macro Average Accuracy:  1.0\n",
      "\n",
      "Macro Average F1-Score:  1.0\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "\n",
      "For K = 3\n",
      "This is the confusion matrix: \n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "\n",
      "Micro Average Precision:  1.0\n",
      "\n",
      "Micro Average Recall:  1.0\n",
      "\n",
      "Micro Average Accuracy:  1.0\n",
      "\n",
      "Micro Average F1-Score:  1.0\n",
      "\n",
      "\n",
      "\n",
      "Macro Average Precision:  1.0\n",
      "\n",
      "Macro Average Recall:  1.0\n",
      "\n",
      "Macro Average Accuracy:  1.0\n",
      "\n",
      "Macro Average F1-Score:  1.0\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "\n",
      "\n",
      "For K = 5\n",
      "This is the confusion matrix: \n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 0 5]]\n",
      "\n",
      "Micro Average Precision:  1.0\n",
      "\n",
      "Micro Average Recall:  1.0\n",
      "\n",
      "Micro Average Accuracy:  1.0\n",
      "\n",
      "Micro Average F1-Score:  1.0\n",
      "\n",
      "\n",
      "\n",
      "Macro Average Precision:  1.0\n",
      "\n",
      "Macro Average Recall:  1.0\n",
      "\n",
      "Macro Average Accuracy:  1.0\n",
      "\n",
      "Macro Average F1-Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluateFinal(prediction, expected):\n",
    "    \n",
    "    truep = [0,0,0]\n",
    "    truen = [0,0,0]\n",
    "    falsep = [0,0,0]\n",
    "    faslen = [0,0,0]\n",
    "    confusion_matrix = np.full((3,3), 0)\n",
    "\n",
    "    for E, P in zip(expected, prediction): \n",
    "        confusion_matrix[P][E] = confusion_matrix[P][E] + 1\n",
    "        \n",
    "    \n",
    "    truep[0] = confusion_matrix[0][0]\n",
    "    falsep[0] = (confusion_matrix[:, 0]).sum() - truep[0]\n",
    "    faslen[0] = (confusion_matrix[0, :]).sum() - truep[0]\n",
    "    truen[0] = (confusion_matrix.sum()) - truep[0] - falsep[0] - faslen[0]\n",
    "    \n",
    "    truep[1] = confusion_matrix[1][1]\n",
    "    falsep[1] = (confusion_matrix[:, 1]).sum() - truep[1]\n",
    "    faslen[1] = (confusion_matrix[1, :]).sum() - truep[1]\n",
    "    truen[1] = (confusion_matrix.sum()) - truep[1] - falsep[1] - faslen[1]\n",
    "        \n",
    "    truep[2] = confusion_matrix[2][2]\n",
    "    falsep[2] = (confusion_matrix[:, 2]).sum() - truep[2]\n",
    "    faslen[2] = (confusion_matrix[2, :]).sum() - truep[2]\n",
    "    truen[2] = (confusion_matrix.sum()) - truep[2] - falsep[2] - faslen[2]\n",
    "    print(\"This is the confusion matrix: \\n\")\n",
    "    print(confusion_matrix)\n",
    "    microA_precision = (sum(truep)/(sum(falsep)+sum(truep)))\n",
    "    microA_recall = (sum(truep)/(sum(faslen)+sum(truep)))\n",
    "    microA_accuracy = ((sum(truep)+sum(truen))/(sum(faslen)+sum(truep)+sum(falsep)+sum(truen)))\n",
    "    microA_f1score = (microA_precision * microA_recall) / (microA_precision + microA_recall)\n",
    "    microA_f1score = microA_f1score * 2\n",
    "    \n",
    "    macroA_precision = []\n",
    "    macroA_recall = []\n",
    "    macroA_accuracy = []\n",
    "    \n",
    "    macroA_precision.append(truep[0] / (truep[0] + falsep[0]))\n",
    "    macroA_recall.append(truep[0] / (truep[0] + faslen[0]))\n",
    "    macroA_accuracy.append((truep[0] + truen[0]) / (truep[0] + truen[0] + falsep[0] + faslen[0]))\n",
    "    \n",
    "    macroA_precision.append(truep[1] / (truep[1] + falsep[1]))\n",
    "    macroA_recall.append(truep[1] / (truep[1] + faslen[1]))\n",
    "    macroA_accuracy.append((truep[1] + truen[1]) / (truep[1] + truen[1] + falsep[1] + faslen[1]))\n",
    "    \n",
    "    macroA_precision.append(truep[2] / (truep[2] + falsep[2]))\n",
    "    macroA_recall.append(truep[2] / (truep[2] + faslen[2]))\n",
    "    macroA_accuracy.append((truep[2] + truen[2]) / (truep[2] + truen[2] + falsep[2] + faslen[2]))\n",
    "\n",
    "    sum_macroA_precision = (sum(macroA_precision))/3\n",
    "    sum_macroA_recall = (sum(macroA_recall))/3\n",
    "    sum_macroA_accuracy = (sum(macroA_accuracy))/3\n",
    "\n",
    "    macroA_f1score = (sum_macroA_precision * sum_macroA_recall) / (sum_macroA_precision + sum_macroA_recall)\n",
    "    macroA_f1score  = 2 * macroA_f1score \n",
    "    \n",
    "    print(\"\\nMicro Average Precision: \", microA_precision)\n",
    "    print(\"\\nMicro Average Recall: \", microA_recall)\n",
    "    print(\"\\nMicro Average Accuracy: \", microA_accuracy)\n",
    "    print(\"\\nMicro Average F1-Score: \", microA_f1score)\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nMacro Average Precision: \", sum_macroA_precision)\n",
    "    print(\"\\nMacro Average Recall: \", sum_macroA_recall)\n",
    "    print(\"\\nMacro Average Accuracy: \", sum_macroA_accuracy)\n",
    "    print(\"\\nMacro Average F1-Score: \", macroA_f1score)\n",
    "    \n",
    "\n",
    "\n",
    "print(\"For K = 1\")\n",
    "evaluateFinal(k1, petal_labels) \n",
    "print(\"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "print(\"\\n\\nFor K = 3\")\n",
    "evaluateFinal(k3, petal_labels)  \n",
    "print(\"\\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n",
    "print(\"\\n\\nFor K = 5\")\n",
    "evaluateFinal(k5, petal_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
